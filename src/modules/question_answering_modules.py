from functools import partial
from typing import Dict, Union, Tuple

import torch
import torch.nn
from torch.nn.init import kaiming_normal_
from torch.nn.modules.module import Module
from torch.utils.tensorboard import SummaryWriter

from src.common.neural_net_param_utils import init_lstm_forget_gate_biases, add_tensor_stats_to_summary_writer
from src.modules.attention.attention_modules import \
    BidirectionalAttention, SymmetricSelfAttention


class QAModuleOutputs:
    def __init__(self, start_index_outputs: torch.Tensor, end_index_outputs: torch.Tensor,
                 supplementary_loss: torch.Tensor = None):
        self.start_index_outputs = start_index_outputs
        self.end_index_outputs = end_index_outputs
        self.supplementary_loss = supplementary_loss


class QAModule(Module):

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__()
        self.embedding = embedding
        self.device = device
        self.summary_writer = summary_writer

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor) -> \
            Union[QAModuleOutputs, Tuple]:
        pass

    def _init_lstm_hidden_and_cell_state(self, batch_size: int, hidden_size: int, is_bidirectional: bool = True,
                                         num_layers: int = 1, as_trainable_params=False):
        return (
            torch.nn.Parameter(torch.zeros((num_layers * (2 if is_bidirectional else 1), batch_size, hidden_size),
                                           device=self.device), requires_grad=True),
            torch.nn.Parameter(torch.zeros((num_layers * (2 if is_bidirectional else 1), batch_size, hidden_size),
                                           device=self.device), requires_grad=True)) if as_trainable_params else (
            torch.zeros((num_layers * (2 if is_bidirectional else 1), batch_size, hidden_size),
                        device=self.device),
            torch.zeros((num_layers * (2 if is_bidirectional else 1), batch_size, hidden_size),
                        device=self.device))

    def log_tensor_stats(self, name_to_tensor_dict: Dict[str, torch.Tensor], iteration_num: int, use_histogram=False):
        for tensor_name, tensor in name_to_tensor_dict.items():
            add_tensor_stats_to_summary_writer(self.summary_writer, tensor, tensor_name, iteration_num,
                                               use_histogram=use_histogram)


class QAModuleWithAttention(QAModule):
    """
        Typical EM accuracy as of #16 - 17 below seems to be 44%+ with 2 epochs, batch-size = 32, grad-accum = 16 steps, Adam with lr = 1e-3 with step-decay
        of 0.1 per epoch. training for 2 epochs

        Observations (2 epoch training):
         1. Sometimes making the model wider is taking a small toll at accuracy. for eg. using a separate bi-attn #2 layer
            for start vs end index results in a lower accuracy by ~ 1%. This could be because we're only training for 2 epochs
         2. Similar to #2, making the final linear layer deal with an input of embedding-size * 9 (obtained by not having the final passage rep lstm
            squeeze the input to half it's size), the accuracy went down by ~3%. the hypothesis is that the final single linear layer has trouble dealing
            with the noise generated by a very large input-size. This might potentially be cured by training for more than 2 epochs (current)
         3. Using asymmetric self-attention as opposed to symmetric everywhere didn't seem to have an accuracy gain. Again this might change when training for more than 2 epochs
         4. Using BertAdam with warmup = 0.1 didn't seem to provide any accuracy gain (rest everything is same including training time of 2 epochs). The only change
            was, gradient-clipping (the default for bertadam) was disabled
         5. 09/25/2019 - for this 2 epoch training, using guided self-attention (using question encoding as a gate when multiplying passage word components)
            didn't yield any gain in accuracy. This was done through a separate model (called QAModuleWithGuidedSelfAttention at the time of this writing).
            The latter was identical to this model except for the use of guided self-attention for passage <-> passage attention
         6. 09/26/2019: trying learnable initial hidden/cell states for final passage rep lstm. no gain in accuracy found for 2 epoch training, test accuracy still lurking
            around ~37%
         7. Added hierarchical attention (passage-self-attention-output X question-embedding concatenated with question-bi-attn-output X passage embedding).
            Accuracy went down by 5% to ~ 32% for 2 epoch training. Squeezing in another batch-norm makes accuracy go up tp 35% in spite of this new attention

            Getting rid of #7's change from here onwards

         8. Feeding the start index outputs while calculating the end index outputs made the EM accuracy go up to 40.98%
         9. Using learned hidden states for the embedding lstm actually decreased accuracy back to ~37% in spite of retaining #8's changes
         10. Using two linear layers at the end with the 1st linear layer squeezing the outputs to embedding_size * 3 (which is when input to the final linear layer) + relu
            after the first linear layer + #8 gave the same accuracy as #8 ~ 40.99%
         11. Adding another bi-attention layer on top of #10 squeezes the accuracy down to 39.37%. this bi-attn layer computes
         another bi-attention on top of 1st bi-attn outputs
         12. Providing a bigger intermediate layer or a bigger 2nd linear output layer decreases performance to around 37%
         13. Going back to #10 (throwing away the bigger intermediate layer in #12 and the 3rd bi-attn layer in #11) but using glove.800b.300d (while only keeping the words/tokens required for squad train/dev datasets) as opposed to the
             available FastText (which isn't the same as FastText 800B) increases the accuracy to 43.91% => embedding quality matters.
             Most likely glove 6B etc. won't be able to reproduce the same results. FastText's authors, in their paper claimed their embedding to be superior to glove's.
             There's a chance that upon using FastText 800B etc., we'd see similar/better improvements
             On a side note, limiting the embedding matrix to only the tokens required for squad, drastically reduces the memory footprint (65 - 70%) -> ~40%
             of V100 memory
         14. Trying two separate LSTMs for the start and end index respectively. this has been tried before with no advantage but the difference this time is, the
             end-index lstm is fed the start-index final output. Accuracy was similar to #13 => 43.82%. The difference seen was, now the start and end index individual accuracies
             are closer to each other 56.x vs 57.y % compared to #13 (~54% and ~58% respectively). Memory usage on V100 went up from ~ 40% to 48.8% compared
             to #13 due to the addition of the new lstm
         15. Same as #14 but using a detach() on the start-index input makes the accuracy go down to 43.42%. The effect detach has here is that the end-index sub-graph training won't
             lead to a change in params for the entire start-index subgraph thus in a way favoring that start-index affect the end-index but the training dedicated to the latter doesn't
             affect the former. Looks like at least for the first 2 epochs, removing this co-adaptation decreases accuracy even if by 0.4 %
         16. Same as #14 (no longer using detach() as noted in 315 above) but initializing unknown word embeddings to torch.randn(300)/100. the accuracy increases to 44.3%.
             When using torch.randn(300)/10 instead, the accuracy goes down to around 43.3% (which is lower than simply initializing
             each unknown word to it's individual 0-tensor)
         17. Adding two embedding lstms with a skip connection between them (2nd lstm input = 1st lstm output + original passage/question batch), accuracy is mostly
             the same as #16 (44.2% as opposed to 44.3% in #16)
         18. Using a leaky-ReLU for the first bidirectional-attention (as opposed to Tanh) led to an accuracy of 43.43. It's comparable to using a Tanh since for one of the runs
             tanh yielded ~43% accuracy
         19. Using bidaf-attention for a 2 epoch training decreased the accuracy to ~40%
         20. Removed bidaf attention and leaky-relu (back to tanh). In the bidrectional attention, it was found that the attention weights for LHS (question) were turning out to be very
             similar for each passage word i.e. the attention paid by each question word to each passage was very homogeneous across passage words. Taking away the
             denominator of sq-root of embedding-size from final_attention_weights_softmaxed_for_lhs made accuracy increase to 44.9%
         21. Using simplified bi-attention (where the lhs and rhs are only multiplied by vectors initialized to 1s as opposed to being passed through linear layers),
             the accuracy observed was 44.3%. For this run, I retained the change where only the rhs aka question-weights for each context word were being divided by
             sq-root of input-size
         22. Using unnormalized weights (i.e. non-softmaxed) even for simplified biattention reduces the accuracy to 40.35%. "even" because the same effect
             was observed for the case of standard bi-attention
         23. Same as #20 (which led to 44.9% accuracy) but using RAdam with lr = 1e-2 (as opposed to be 1e-3 for most other runs that used Adam, including #20). Interestingly,
             using 1e-3 with RAdam led to very high errors (0.3+) even until iteration #4000+ (stopped this run before it completed after seeing this).
             Also, the avg. loss was close (in fact lower at the 4th place of decimal) to #20. 0.2363 for #20 vs 0.2360 for RAdam. Looks like RAdam
             will allow a higher learning rate as reported in online literature/discussion-threads. This might prove useful for > 2 epoch training.
             Verified again that using 1e-2 with Adam is disastrous as expected from very early runs (loss = 0.48 around iteration# 3900).
             This was worse than using 1e-3 with RAdam
         24. On a slightly different note, tried AdamW as well. first with weight-decay = 1e-2 (this run was using the simplified bi-attention) => accuracy was
             around 40%. However, when beefing up the regularization to 5e-2 (and switching to #20 architecture i.e. going back to standard bi-attention),
             the dev accuracy went down to 27% => let's avoid using too high of a weight decay right now at least for the 2 epoch training
         25. Using a larger hidden state on the embedding lstm (350 as opposed to 300) increases EM accuracy to 44.94%. This also involved a bug-fix
             which was caused by pytorch’s new BoolTensor behavior which was making the accuracy seem like 56.68% which was identical to the start-index accuracy.
             Using a hidden state of 400 decreases the accuracy to 43.7%
         26. Using a forget gate bias of 1 (=> remember everything) on the embedding lstm decreased the accuracy to 40.8% in spite of using the better hidden size of
             350
         27. wiring the self-attention output to linear_layer_before_final_passage_rep decreases the accuracy to 43.6% while wiring it to the final linear layers
             (linear_layer_for_start/end_index) decreases the accuracy to 40.*%
         28. Added a question encoder which uses max-pooling(similar to infersent), the accuracy was around 40.45% in 2 epoch training. Refer to QAModuleWithAttentionLargeWithQuestionEncoder.
             It seems 2 epochs aren't enough for question encoding to help
         29. Using a shared self-attention module between the question and the passage brings down the accuracy to 42.7%
             compared to #25's 44.9%. This used the same embedding-lstm hidden size which brought 44.9% in #25
         30. Using a smaller linear layer for self-attention (embedding-size -> embedding-size/2) decreases the accuracy to 43.7%
             (compared to 44.9% without this change i.e. using the original model in 25%)
         31. From the base model in #25 (44.9% EM), removing the self-attention linear layer bias decreased the accuracy to 43.03%
         32. Compared to the base-model, removing self-attention altogether and simply feeding the 1st bi-attn output as the 2nd bi-attn input leads
             to an accuracy of ~42.9%. since this is sort of an ablation study, the memory usage goes down
         33. Replacing self-attention by an enhanced self-attention which uses input * input * lambda + input * (max-from_input-along_batch_dimension - input) * (1 - lambda)
             as opposed to input * input, accuracy goes down to 42.09. While this design may be a good idea, we see that the lambda (which is a vector of size = input-size)
             values when initialized to 0.5, are only able to move by 0.05 or so indicating that we either need longer training times or a higher learning rate. Take a look
             at results/using_enhanced_self_attention_type_1
         34. Same as #33 but the input * input  coefficients (a vector of size. = input_size) were initialized to 0.9  instead of 0.5. The accuracy goes up to 44.66% as opposed
             to 42.09 for the 0.5 case. This is now close to the original self-attention-only variant which reached 44.9% (which was achieved in #25)
         35. Tried the pytorch Transformer as a replacement for the self-attentions. The same 2x2 transformer (2 TransformerEncoderLayers with 2 attenton heads each)
             was used for both question and passage. Accuracy went down to 40.3%. Take a look at using_a_2x2_tranformer_for_self_attention.rtf
         36. Using simplified self-attention: Non symmetric Simplified self-attention (using lhs and rhs vector multipliers params as opposed to passing the input through a linear layer.
             The params were initialized using  ~ N(1, 0.0001) i.e. std = 0.01, accuracy went down to 42.29%.
         37. Since switching to kaiming_uniform(default for linear layers) from wherever it was kaiming_normal or xavier_normal, the #25 model's accuracy
             has been lurking around ~42% for the last 2 runs. Removing the norm layer right after final_concatenated_output increased the accuracy to
             43%+
         38. Interestingly, while having a batch-size of 50 has generally given a lower accuracy compared to batch-size = 32 indicating
             that a higher batch-size isn't necessarily better. In one of the most recent runs, increasing the batch-size to 64 increased
             the accuracy to 44.7% (note that after the removal of normal initialization, the accuracy had been lurking around 42 - 43%
             with batch-size = 32).
         39. As noted in #38, having the batch-norm layer right after final-concatenated-output ( as of this writing in the original model this
             = [bi-attn-output1, bi-attn-output-2]) is not yielding any accuracy gains. In fact, in one of the recent experiments, removing this layer
             led to around a 1% gain in accuracy. Note that this world is slightly different since kaiming_normal/xavier_normal have been removed. There's
             a chance putting them back could change the situation a bit
         40. If the batch-norm layer after the embedding linear layer is moved before the linear layer, the accuracy goes down to 39.88%
         41. If tanh is added after embedding linear layer in #40, the accuracy decreases drastically => ~21%. It can be found commented out
             in the "...didn't_work_well" file with the name "QAModuleWithAttentionTanhAfterEmbedding"
         42. Removing the changes made in #40 and #41 and going back to the original model in #37 (with batch-norm removed). Tried 1 epoch training
             with batch-size = 32 and grad-accumulation steps = 16. The accuracy was already 39.54% => the 2nd epoch isn't helping all that much
             perhaps.
         43. While doing #42, also logged the activations. Interestingly, while the max-value for an embedding scalar from GLoVE is around 4.x,
             after applying our linear + batch-norm layers to those embeddings, the max-values can shoot upto 9 - 10. Some local runs suggest that
             this may be due to the batch-norm layer diving the original values by a < 1 variance. While the hypothesis is that it should negatively affect
             the results, it's not clear yet to what extent and how to get rid of this without removing the embedding batch-norm layer
         44. On a slightly different note, did some local runs with a sample dataset of size ~90. It seems for a simple 1 epoch training on this very tiny dataset, kaiming_normal
             init for the linear layer for embedding (called linear_layer_for_final_embedding as of this writing) while not using a bias
             may yield better performance => subject to experimentation on the larger dataset. Note though that this can be misleading
             as even the tanh-after-embedding option (which underperformed in #41) can look comparable to other approaches for a 1 epoch local
             run with a tiny dataset

            TODO: make batch-size a constructor param?

            1. Layer-wise learning rates: https://stackoverflow.com/questions/51801648/how-to-apply-layer-wise-learning-rate-in-pytorch
            2. evaluate saved models: https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/32

        Conneau et al (https://arxiv.org/pdf/1705.02364.pdf, InferSent) had an interesting observation that while sentence representation models trained using attention
        can fit faster, LSTM models could generalize better to other tasks.

        In their case a BI-LSTM with max-pooling between the last forward and backward hidden states yielded the best sentence rep.
        The caveat is, these models have been trained on SNLI and possibly for a lot of epochs. This paper doesn't mention the
        exact number of epochs, simply that they conditionally decrease the lr whenever the dev accuracy after an epoch goes down and stop training once the lr
        goes below 1e-5

        45. If we "bubble-up" self-attention outputs i.e. include them in the final concatenated passage rep while reducing their dimension
            to not include their inputs (since in final passage rep, that's already included through bi-attn1), the accuracy goes down slightly
            to 40%. In this case the final passage rep dimensions were preserved to be identical to earlier to ascertain this effect
            wasn't due to the training needing to deal with a higher dimensional input
        46. There was an experiment which didn't have the norm_layer_before_final_passage_rep (batch-norm right before final passage rep
            is fed to the final LSTM). The accuracy decreased drastically to 22% in the std 2 epoch training
        47. In another experiment, the self-attn was fed bi-attn-1's outputs, accuracy went down to 18% => mixing
            self-attn on top of bi-attn won't work at least with a 2-epoch training
        48. Using 3 bi-attns (and no self-attn) yields interesting results: With activations = tanh, relu and prelu (with a default a = 0.25)
            the accuracy is mere 39.2% while if prelu is replaced with a gelu, the accuracy reaches 42.3%. From tensorboard plots, it was
            seen that the prelu 'a' param (refer to pytorch docs), reduced to around 0.15 during 2 epoch training. Then, when initializing
            PReLU's 'a'/'init' to 0.15, the accuracy went up to ~42.05
        49. Having 2 bi-attns only (1 relu and one tanh + no self attention. 2nd bi-attn also receives final embedding input directly)
            decreases the accuracy to 40.4%
        50. Using modified self-attention. In this case modified = the self-attention's (linear layer + activation) were applied after
            doing the attention calculation. The accuracy goes down to 39.6% for 2-epoch training.
        51. Tried new trimmed glove vectors which do not have a special embedding for ‘.’ and use the average of
            character embeddings for unknown words. Accuracy went down a bit to 41.4% for the original model compared to 42%
            using old trimmed vectors which used torch.rand(300)/100 for unknown words
        52. Using mod5 (removing final passage rep lstm bias) led to 46% and 44.7% acc in two respective runs which is
            higher than currently being achieved by other models. Also using a cumulative batch-size of 256
            (batch-size = 32 with 8 grad accum. steps) now. StepLR, 2 epoch training
        53. At this point, it seems like CyclicLR with Adam isn't working as well as StepLR for any of the models. Often seems
            to overfit in 3rd and 4th epoch
    """

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=True)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=True)

        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_start_index, value=1.0)
        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_end_index, value=1.0)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)
        question_embedding = torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training and False:
            self.log_tensor_stats({"passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleWithAttentionMod2(QAModule):
    """
        using self-attn after 2nd bi-attn
    """

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=True)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=True)

        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_start_index, value=1.0)
        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_end_index, value=1.0)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)
        question_embedding = torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # (N, length, embedding_size) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_embedding, passage_embedding)

        # each is (N, length, 2 * embedding_size). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_bidirectional_attention_output_2)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_bidirectional_attention_output_2)

        # (N, SEQUENCE_LENGTH, 2 * embedding_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output, passage_self_bi_att_output], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training and False:
            self.log_tensor_stats({"passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleWithAttentionMod3(QAModule):
    """
    A change from the original model, using batch-norm for the 1st bi-attn, accuracy increases to 43.6% compared to
    original model's 42%+
    """

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False,
                                                                     use_batch_norm=True)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False,
                                                                       use_batch_norm=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=True)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=True)

        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_start_index, value=1.0)
        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_end_index, value=1.0)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)
        question_embedding = torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training and False:
            self.log_tensor_stats({"passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleWithAttentionMod4(QAModule):
    """
    minor tweaks, changed self-attention init to kaiming normal and removed the linear_layer_before_final_passage_rep bias.
    It initially gave 42% acc but after removing the bias in final linear layers for start/end index, the acc reduced to 40.5%
    """

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=partial(kaiming_normal_,
                                                                                                      nonlinearity='relu'))

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=partial(kaiming_normal_,
                                                                                                    nonlinearity='relu'))

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2, bias=False)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=True)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=True)

        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_start_index, value=1.0)
        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_end_index, value=1.0)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1, bias=False)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1, bias=False)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)
        question_embedding = torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training and False:
            self.log_tensor_stats({"passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleWithAttentionMod5(QAModule):
    """ removed the final passage rep start/end index lstm biases. Error 41.17 with CyclicLR but with stepLR the acc is 46.08%
     in one run and 44.7 in another"""

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=False)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=False)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)
        question_embedding = torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training:
            self.log_tensor_stats({"embedding_weights": self.embedding.weight,
                                   "embedding_linear_layer": self.linear_layer_for_final_embedding.weight,
                                   "bi_attn1_linear_weight/lhs": self.bidirectional_attention_module.linear_layer_lhs.weight,
                                   "bi_attn1_linear_weight/rhs": self.bidirectional_attention_module.linear_layer_rhs.weight,
                                   "question_self_attn_linear_weight": self.question_to_question_attention.linear_layer.weight,
                                   "passage_self_attn_linear_weight": self.passage_to_passage_attention.linear_layer.weight,
                                   "bi_attn2_linear_weight/lhs": self.bidirectional_attention_module_2.linear_layer_lhs.weight,
                                   "bi_attn2_linear_weight/rhs": self.bidirectional_attention_module_2.linear_layer_rhs.weight,
                                   "norm_layer_for_embedding_output": self.norm_layer_for_embedding_output.weight,
                                   "norm_layer_before_final_passage_rep": self.norm_layer_before_final_passage_rep.weight,
                                   "start_index_batch_norm_layer": self.start_index_batch_norm_layer.weight,
                                   "end_index_batch_norm_layer": self.end_index_batch_norm_layer.weight,
                                   "linear_layer_for_start_index": self.linear_layer_for_start_index.weight,
                                   "linear_layer_for_end_index": self.linear_layer_for_end_index.weight,
                                   "passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num, use_histogram=True)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleWithAttentionMod5StaticBatchNorms(QAModule):
    """ Same as mod5 but batch-norm layers are static i.e. affine=False"""

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size, affine=False)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2, affine=False)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=False)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=False)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5, affine=False)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5, affine=False)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)
        question_embedding = torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training and (iteration_num == 1 or iteration_num % 1000 == 0):
            self.log_tensor_stats({"embedding_weights": self.embedding.weight,
                                   "embedding_linear_layer": self.linear_layer_for_final_embedding.weight,
                                   "bi_attn1_linear_weight/lhs": self.bidirectional_attention_module.linear_layer_lhs.weight,
                                   "bi_attn1_linear_weight/rhs": self.bidirectional_attention_module.linear_layer_rhs.weight,
                                   "question_self_attn_linear_weight": self.question_to_question_attention.linear_layer.weight,
                                   "passage_self_attn_linear_weight": self.passage_to_passage_attention.linear_layer.weight,
                                   "bi_attn2_linear_weight/lhs": self.bidirectional_attention_module_2.linear_layer_lhs.weight,
                                   "bi_attn2_linear_weight/rhs": self.bidirectional_attention_module_2.linear_layer_rhs.weight,
                                   "linear_layer_for_start_index": self.linear_layer_for_start_index.weight,
                                   "linear_layer_for_end_index": self.linear_layer_for_end_index.weight,
                                   "passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num, use_histogram=True)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleMod5StaticBatchNormsNoBNBeforeFinalLinearLayers(QAModule):
    """ Same as mod5 but batch-norm layers are static i.e. affine=False but no batch norm right before
     final linear layers"""

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size, affine=False)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2, affine=False)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=False)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=False)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)
        question_embedding = torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_outputs_for_start_index = self.linear_layer_for_start_index(final_enriched_passage_output_start_index)

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_outputs_for_end_index = self.linear_layer_for_end_index(final_enriched_passage_output_end_index)

        if self.training and (iteration_num == 1 or iteration_num % 1000 == 0):
            self.log_tensor_stats({"embedding_weights": self.embedding.weight,
                                   "embedding_linear_layer": self.linear_layer_for_final_embedding.weight,
                                   "bi_attn1_linear_weight/lhs": self.bidirectional_attention_module.linear_layer_lhs.weight,
                                   "bi_attn1_linear_weight/rhs": self.bidirectional_attention_module.linear_layer_rhs.weight,
                                   "question_self_attn_linear_weight": self.question_to_question_attention.linear_layer.weight,
                                   "passage_self_attn_linear_weight": self.passage_to_passage_attention.linear_layer.weight,
                                   "bi_attn2_linear_weight/lhs": self.bidirectional_attention_module_2.linear_layer_lhs.weight,
                                   "bi_attn2_linear_weight/rhs": self.bidirectional_attention_module_2.linear_layer_rhs.weight,
                                   "linear_layer_for_start_index": self.linear_layer_for_start_index.weight,
                                   "linear_layer_for_end_index": self.linear_layer_for_end_index.weight,
                                   "passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num, use_histogram=True)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleMod5StaticBatchNormsNoBNBeforeFinalLinearLayersWDropout(QAModule):
    """ Same as mod5 but batch-norm layers are static i.e. affine=False but no batch norm right before
     final linear layers and a dropout layer after final embedding"""

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size, affine=False)
        self.dropout_for_embedding_output = torch.nn.Dropout(p=0.2)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2, affine=False)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=False)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=False)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = self.dropout_for_embedding_output(torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2))
        question_embedding = self.dropout_for_embedding_output(torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2))

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_outputs_for_start_index = self.linear_layer_for_start_index(final_enriched_passage_output_start_index)

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_outputs_for_end_index = self.linear_layer_for_end_index(final_enriched_passage_output_end_index)

        if self.training and (iteration_num == 1 or iteration_num % 1000 == 0):
            self.log_tensor_stats({"embedding_weights": self.embedding.weight,
                                   "embedding_linear_layer": self.linear_layer_for_final_embedding.weight,
                                   "bi_attn1_linear_weight/lhs": self.bidirectional_attention_module.linear_layer_lhs.weight,
                                   "bi_attn1_linear_weight/rhs": self.bidirectional_attention_module.linear_layer_rhs.weight,
                                   "question_self_attn_linear_weight": self.question_to_question_attention.linear_layer.weight,
                                   "passage_self_attn_linear_weight": self.passage_to_passage_attention.linear_layer.weight,
                                   "bi_attn2_linear_weight/lhs": self.bidirectional_attention_module_2.linear_layer_lhs.weight,
                                   "bi_attn2_linear_weight/rhs": self.bidirectional_attention_module_2.linear_layer_rhs.weight,
                                   "linear_layer_for_start_index": self.linear_layer_for_start_index.weight,
                                   "linear_layer_for_end_index": self.linear_layer_for_end_index.weight,
                                   "passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num, use_histogram=True)

        return final_outputs_for_start_index, final_outputs_for_end_index



class QAModuleWithAttentionMod5StaticBatchNormsNoBNAfterEmbedding(QAModule):
    """ Same as QAModuleWithAttentionMod5StaticBatchNorms but no batch norm for embedding"""

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2, affine=False)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=False)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=False)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5, affine=False)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5, affine=False)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training and (iteration_num == 1 or iteration_num % 1000 == 0):
            self.log_tensor_stats({"embedding_weights": self.embedding.weight,
                                   "embedding_linear_layer": self.linear_layer_for_final_embedding.weight,
                                   "bi_attn1_linear_weight/lhs": self.bidirectional_attention_module.linear_layer_lhs.weight,
                                   "bi_attn1_linear_weight/rhs": self.bidirectional_attention_module.linear_layer_rhs.weight,
                                   "question_self_attn_linear_weight": self.question_to_question_attention.linear_layer.weight,
                                   "passage_self_attn_linear_weight": self.passage_to_passage_attention.linear_layer.weight,
                                   "bi_attn2_linear_weight/lhs": self.bidirectional_attention_module_2.linear_layer_lhs.weight,
                                   "bi_attn2_linear_weight/rhs": self.bidirectional_attention_module_2.linear_layer_rhs.weight,
                                   "linear_layer_for_start_index": self.linear_layer_for_start_index.weight,
                                   "linear_layer_for_end_index": self.linear_layer_for_end_index.weight,
                                   "passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num, use_histogram=True)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleMod5StaticBatchNormsNoBNAfterEmbeddingOrBeforeLinearLayers(QAModule):
    """ Same as QAModuleWithAttentionMod5StaticBatchNorms but no batch norm for embedding and no batch norm right before
     final linear layers"""

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2, affine=False)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=False)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=False)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_outputs_for_start_index = self.linear_layer_for_start_index(final_enriched_passage_output_start_index)

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_outputs_for_end_index = self.linear_layer_for_end_index(final_enriched_passage_output_end_index)

        if self.training and (iteration_num == 1 or iteration_num % 1000 == 0):
            self.log_tensor_stats({"embedding_weights": self.embedding.weight,
                                   "embedding_linear_layer": self.linear_layer_for_final_embedding.weight,
                                   "bi_attn1_linear_weight/lhs": self.bidirectional_attention_module.linear_layer_lhs.weight,
                                   "bi_attn1_linear_weight/rhs": self.bidirectional_attention_module.linear_layer_rhs.weight,
                                   "question_self_attn_linear_weight": self.question_to_question_attention.linear_layer.weight,
                                   "passage_self_attn_linear_weight": self.passage_to_passage_attention.linear_layer.weight,
                                   "bi_attn2_linear_weight/lhs": self.bidirectional_attention_module_2.linear_layer_lhs.weight,
                                   "bi_attn2_linear_weight/rhs": self.bidirectional_attention_module_2.linear_layer_rhs.weight,
                                   "linear_layer_for_start_index": self.linear_layer_for_start_index.weight,
                                   "linear_layer_for_end_index": self.linear_layer_for_end_index.weight,
                                   "passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num, use_histogram=True)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleStaticBatchNormsNoBNAfterEmbeddingQuestionEncoder(QAModule):
    """ Same as QAModuleWithAttentionMod5StaticBatchNorms but no batch norm for embedding and a question encoder-decoder
    with question embedding fed with the final_concatenated_passage_output to the
    lstm_for_final_passage_representation_start/end_index"""

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2, affine=False)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 3,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=False)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 3 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=False)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5, affine=False)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5, affine=False)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

        self.question_encoder = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding.embedding_dim,
                                              bidirectional=False, batch_first=True, bias=False)

        self.question_decoder = torch.nn.LSTM(input_size=self.embedding.embedding_dim,
                                              hidden_size=self.embedding.embedding_dim, bidirectional=False,
                                              batch_first=True, bias=False)

        self.question_encoding_loss = torch.nn.MSELoss()

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        ######## question encoder ######
        init_hidden_and_cell_state_for_encoder = self._init_lstm_hidden_and_cell_state(question_embedding.size()[0],
                                                                                       question_embedding.size()[2],
                                                                                       is_bidirectional=False)

        # note that we're only taking the final hidden state here. the output of lstm is "outputs, (h_n, c_n)"
        encoded_question_tensor = torch.transpose(self.question_encoder(question_embedding,
                                                                        init_hidden_and_cell_state_for_encoder)[1][
                                                      0], dim0=0, dim1=1)

        # let's initialize it to zeroes too, we'll change it as we decode
        init_hidden_and_cell_state_for_decoder = init_hidden_and_cell_state_for_encoder
        question_decoder_outputs = []

        for _ in torch.split(question_embedding, 1, dim=1):
            decoder_output, decoder_hidden_and_cell_state = self.question_decoder(encoded_question_tensor,
                                                                                  init_hidden_and_cell_state_for_decoder)
            question_decoder_outputs.append(decoder_output)
            init_hidden_and_cell_state_for_decoder = decoder_hidden_and_cell_state

        question_decoder_outputs_concatenated = torch.cat(question_decoder_outputs, dim=1)

        assert question_decoder_outputs_concatenated.size() == question_embedding.size()

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        # lets' have both bi-attns' outputs + question encoding
        final_concatenated_passage_output = torch.cat([torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2),
            encoded_question_tensor.expand(-1, passage_embedding.size()[1], -1)], dim=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training and (iteration_num == 1 or iteration_num % 1000 == 0):
            self.log_tensor_stats({"embedding_weights": self.embedding.weight,
                                   "embedding_linear_layer": self.linear_layer_for_final_embedding.weight,
                                   "bi_attn1_linear_weight/lhs": self.bidirectional_attention_module.linear_layer_lhs.weight,
                                   "bi_attn1_linear_weight/rhs": self.bidirectional_attention_module.linear_layer_rhs.weight,
                                   "question_self_attn_linear_weight": self.question_to_question_attention.linear_layer.weight,
                                   "passage_self_attn_linear_weight": self.passage_to_passage_attention.linear_layer.weight,
                                   "bi_attn2_linear_weight/lhs": self.bidirectional_attention_module_2.linear_layer_lhs.weight,
                                   "bi_attn2_linear_weight/rhs": self.bidirectional_attention_module_2.linear_layer_rhs.weight,
                                   "linear_layer_for_start_index": self.linear_layer_for_start_index.weight,
                                   "linear_layer_for_end_index": self.linear_layer_for_end_index.weight,
                                   "passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num, use_histogram=True)

        return final_outputs_for_start_index, final_outputs_for_end_index, self.question_encoding_loss(
            question_decoder_outputs_concatenated, question_embedding)


class QAModuleStaticBatchNormsNoBNAfterEmbeddingQuestionEncoder2(QAModule):
    """ Same as QAModuleWithAttentionMod5StaticBatchNorms but no batch norm for embedding and a question encoder-decoder
    with question embedding fed to the final linear layers"""

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size * 2,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2, affine=False)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=False)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=False)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5, affine=False)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5, affine=False)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 6, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 6, 1)

        self.question_encoder = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding.embedding_dim,
                                              bidirectional=False, batch_first=True, bias=False)

        self.question_decoder = torch.nn.LSTM(input_size=self.embedding.embedding_dim,
                                              hidden_size=self.embedding.embedding_dim, bidirectional=False,
                                              batch_first=True, bias=False)

        self.question_encoding_loss = torch.nn.MSELoss()

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_embedding)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_embedding)

        # (N, length, 2 * INPUT_SIZE) each
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_self_bi_att_output, passage_self_bi_att_output)

        ######## question encoder ######
        init_hidden_and_cell_state_for_encoder = self._init_lstm_hidden_and_cell_state(question_embedding.size()[0],
                                                                                       question_embedding.size()[2],
                                                                                       is_bidirectional=False)

        # note that we're only taking the final hidden state here. the output of lstm is "outputs, (h_n, c_n)"
        encoded_question_tensor = torch.transpose(self.question_encoder(question_embedding,
                                                                        init_hidden_and_cell_state_for_encoder)[1][
                                                      0], dim0=0, dim1=1)

        # let's initialize it to zeroes too, we'll change it as we decode
        init_hidden_and_cell_state_for_decoder = init_hidden_and_cell_state_for_encoder
        question_decoder_outputs = []

        for _ in torch.split(question_embedding, 1, dim=1):
            decoder_output, decoder_hidden_and_cell_state = self.question_decoder(encoded_question_tensor,
                                                                                  init_hidden_and_cell_state_for_decoder)
            question_decoder_outputs.append(decoder_output)
            init_hidden_and_cell_state_for_decoder = decoder_hidden_and_cell_state

        question_decoder_outputs_concatenated = torch.cat(question_decoder_outputs, dim=1)

        assert question_decoder_outputs_concatenated.size() == question_embedding.size()

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output,
                       passage_bidirectional_attention_output_2], dim=2))

        # lets' have both bi-attns' outputs + question encoding
        final_concatenated_passage_output = torch.cat([torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)], dim=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.cat([
                encoded_question_tensor.expand(-1, passage_embedding.size()[1], -1),
                torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2)], dim=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.cat([
                encoded_question_tensor.expand(-1, passage_embedding.size()[1], -1),
                torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2)], dim=2))

        if self.training and (iteration_num == 1 or iteration_num % 1000 == 0):
            self.log_tensor_stats({"embedding_weights": self.embedding.weight,
                                   "embedding_linear_layer": self.linear_layer_for_final_embedding.weight,
                                   "bi_attn1_linear_weight/lhs": self.bidirectional_attention_module.linear_layer_lhs.weight,
                                   "bi_attn1_linear_weight/rhs": self.bidirectional_attention_module.linear_layer_rhs.weight,
                                   "question_self_attn_linear_weight": self.question_to_question_attention.linear_layer.weight,
                                   "passage_self_attn_linear_weight": self.passage_to_passage_attention.linear_layer.weight,
                                   "bi_attn2_linear_weight/lhs": self.bidirectional_attention_module_2.linear_layer_lhs.weight,
                                   "bi_attn2_linear_weight/rhs": self.bidirectional_attention_module_2.linear_layer_rhs.weight,
                                   "linear_layer_for_start_index": self.linear_layer_for_start_index.weight,
                                   "linear_layer_for_end_index": self.linear_layer_for_end_index.weight,
                                   "passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num, use_histogram=True)

        return final_outputs_for_start_index, final_outputs_for_end_index, self.question_encoding_loss(
            question_decoder_outputs_concatenated, question_embedding)


class QAModuleWithAttentionMod6(QAModule):
    """
    putting self-attention after bi-attn 2, 43.85% with stepLR and 2 epoch. since other models are only hitting 42 - 43% at this point,
    this seems better for 2 epoch training but it overfits a bit for 4 epochs and the acc went down to 40.65
    """

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=True)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=True)

        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_start_index, value=1.0)
        init_lstm_forget_gate_biases(self.lstm_for_final_passage_representation_end_index, value=1.0)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)
        question_embedding = torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # (N, seq_length,  embedding_size)
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_bidirectional_attention_output_2)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_bidirectional_attention_output_2)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output, passage_self_bi_att_output], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training and False:
            self.log_tensor_stats({"passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num)

        return final_outputs_for_start_index, final_outputs_for_end_index


class QAModuleWithAttentionMod7(QAModule):
    """
    putting self-attention after bi-attn 2 + removing final lstm bias
    """

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)
        self.embedding_lstm_hidden_size = int(self.embedding.embedding_dim + self.embedding.embedding_dim / 6)
        self.lstm_for_embedding = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding_lstm_hidden_size,
                                                bidirectional=True, batch_first=True, bias=False)

        self.final_embedding_size = int(self.embedding.embedding_dim)

        self.linear_layer_for_final_embedding = torch.nn.Linear(
            self.embedding.embedding_dim + self.embedding_lstm_hidden_size * 2,
            self.final_embedding_size, bias=False)

        self.norm_layer_for_embedding_output = torch.nn.BatchNorm1d(self.final_embedding_size)

        self.bidirectional_attention_module = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                     activation=torch.nn.Tanh,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights_for_rhs=True,
                                                                     scale_attention_weights_for_lhs=False)

        self.bidirectional_attention_module_2 = BidirectionalAttention(input_size=self.final_embedding_size,
                                                                       return_with_inputs_concatenated=False,
                                                                       activation=torch.nn.Tanh,
                                                                       scale_attention_weights_for_rhs=True,
                                                                       scale_attention_weights_for_lhs=False)

        self.question_to_question_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                     return_with_inputs_concatenated=True,
                                                                     scale_attention_weights=True,
                                                                     activation=torch.nn.ReLU,
                                                                     linear_layer_weight_init=kaiming_normal_)

        self.passage_to_passage_attention = SymmetricSelfAttention(input_size=self.final_embedding_size,
                                                                   return_with_inputs_concatenated=True,
                                                                   scale_attention_weights=True,
                                                                   activation=torch.nn.ReLU,
                                                                   linear_layer_weight_init=kaiming_normal_)

        self.linear_layer_before_final_passage_rep = torch.nn.Linear(self.final_embedding_size * 4,
                                                                     self.final_embedding_size * 2)

        self.norm_layer_before_final_passage_rep = torch.nn.BatchNorm1d(self.final_embedding_size * 2)

        self.lstm_for_final_passage_representation_start_index = torch.nn.LSTM(self.final_embedding_size * 2,
                                                                               self.final_embedding_size * 2,
                                                                               bidirectional=True, batch_first=True,
                                                                               bias=False)

        self.lstm_for_final_passage_representation_end_index = torch.nn.LSTM(self.final_embedding_size * 2 + 1,
                                                                             self.final_embedding_size * 2,
                                                                             bidirectional=True, batch_first=True,
                                                                             bias=False)

        self.lstm_for_final_passage_rep_init_hidden_state, self.lstm_for_final_passage_rep_init_cell_state = \
            self._init_lstm_hidden_and_cell_state(1, self.final_embedding_size * 2, is_bidirectional=True,
                                                  as_trainable_params=True)

        self.start_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)
        self.end_index_batch_norm_layer = torch.nn.BatchNorm1d(self.final_embedding_size * 5)

        self.linear_layer_for_start_index = torch.nn.Linear(self.final_embedding_size * 5, 1)
        self.linear_layer_for_end_index = torch.nn.Linear(self.final_embedding_size * 5, 1)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        # pdb.set_trace()

        question_lstm_output = self.lstm_for_embedding(question_batch, self._init_lstm_hidden_and_cell_state(
            question_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_lstm_output = self.lstm_for_embedding(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding_lstm_hidden_size))[0]

        passage_embedding_input = torch.cat([passage_batch, passage_lstm_output], dim=2)
        question_embedding_input = torch.cat([question_batch, question_lstm_output], dim=2)

        passage_embedding_input = self.linear_layer_for_final_embedding(passage_embedding_input)
        question_embedding_input = self.linear_layer_for_final_embedding(question_embedding_input)

        # (N, seq_length, embedding_size)
        passage_embedding = torch.transpose(self.norm_layer_for_embedding_output(
            torch.transpose(passage_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)
        question_embedding = torch.transpose(self.norm_layer_for_embedding_output(torch.transpose(
            question_embedding_input, dim0=1, dim1=2)), dim0=1, dim1=2)

        # (N, seq_length,  2 * embedding_size)
        question_bidirectional_attention_output, passage_bidirectional_attention_output = \
            self.bidirectional_attention_module(question_embedding, passage_embedding)

        # (N, seq_length,  embedding_size)
        question_bidirectional_attention_output_2, passage_bidirectional_attention_output_2 = \
            self.bidirectional_attention_module_2(question_embedding, passage_embedding)

        # each is (N, length, 2 * INPUT_SIZE). 2 * because by-default the bi-att module concatenates
        # input with the bi-att output
        question_self_bi_att_output = self.question_to_question_attention(question_bidirectional_attention_output_2)
        passage_self_bi_att_output = self.passage_to_passage_attention(passage_bidirectional_attention_output_2)

        # (N, SEQUENCE_LENGTH, 2 * input_size)
        final_concatenated_passage_output = self.linear_layer_before_final_passage_rep(
            torch.cat([passage_bidirectional_attention_output, passage_self_bi_att_output], dim=2))

        final_concatenated_passage_output = torch.transpose(self.norm_layer_before_final_passage_rep(
            torch.transpose(final_concatenated_passage_output, dim0=1, dim1=2)), dim0=1, dim1=2)

        init_hidden_state_repeated = self.lstm_for_final_passage_rep_init_hidden_state.repeat(1,
                                                                                              final_concatenated_passage_output.size()[
                                                                                                  0], 1)
        init_cell_state_repeated = self.lstm_for_final_passage_rep_init_cell_state.repeat(1,
                                                                                          final_concatenated_passage_output.size()[
                                                                                              0], 1)

        # start index
        final_enriched_passage_output_start_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_start_index(final_concatenated_passage_output,
                                                                    (init_hidden_state_repeated,
                                                                     init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_start_index = torch.transpose(final_enriched_passage_output_start_index, dim0=1,
                                                                    dim1=2)

        batch_normed_start_index_output = self.start_index_batch_norm_layer(final_enriched_passage_output_start_index)
        final_outputs_for_start_index = self.linear_layer_for_start_index(
            torch.transpose(batch_normed_start_index_output, dim0=1, dim1=2))

        # end index
        final_enriched_passage_output_end_index = torch.cat(
            [passage_embedding,
             self.lstm_for_final_passage_representation_end_index(torch.cat([final_concatenated_passage_output,
                                                                             final_outputs_for_start_index], dim=2),
                                                                  (init_hidden_state_repeated,
                                                                   init_cell_state_repeated))[0]], dim=2)

        final_enriched_passage_output_end_index = torch.transpose(final_enriched_passage_output_end_index, dim0=1,
                                                                  dim1=2)
        batch_normed_end_index_output = self.end_index_batch_norm_layer(final_enriched_passage_output_end_index)
        final_outputs_for_end_index = self.linear_layer_for_end_index(
            torch.transpose(batch_normed_end_index_output, dim0=1, dim1=2))

        if self.training and False:
            self.log_tensor_stats({"passage/embedding_input": passage_embedding_input,
                                   "question/embedding_input": question_embedding_input,
                                   "passage/embedding": passage_embedding, "question/embedding": question_embedding,
                                   "passage/bi_attn": passage_bidirectional_attention_output,
                                   "question/bi_attn": question_bidirectional_attention_output,
                                   "passage/self_attn": passage_self_bi_att_output,
                                   "question/self_attn": question_self_bi_att_output,
                                   "passage/bi_attn_2": passage_bidirectional_attention_output_2,
                                   "passage/final_enriched_output_start_idx": final_enriched_passage_output_start_index,
                                   "passage/batch_normed_start_index_output": batch_normed_start_index_output,
                                   "passage/final_outputs_for_start_index": final_outputs_for_start_index,
                                   "passage/final_enriched_passage_output_end_index": final_enriched_passage_output_end_index,
                                   "passage/batch_normed_end_index_output": batch_normed_end_index_output,
                                   "passage/final_outputs_for_end_index": final_outputs_for_end_index
                                   }, iteration_num)

        return final_outputs_for_start_index, final_outputs_for_end_index


class BasicAttentiveReader(QAModule):
    """
    A simple model as described here: https://www.youtube.com/watch?v=yIdF-17HwSk to use as a baseline.
    Starts around 50 mins into the video. Very low 2-epoch acc (~17% or 19% from what I recall)
    """

    def __init__(self, embedding: torch.nn.Embedding, device, summary_writer: SummaryWriter = None):
        super().__init__(embedding=embedding, device=device, summary_writer=summary_writer)

        self.question_embedding_lstm = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding.embedding_dim,
                                                     bidirectional=True, batch_first=True, bias=False)

        self.passage_lstm = torch.nn.LSTM(self.embedding.embedding_dim, self.embedding.embedding_dim,
                                          bidirectional=True, batch_first=True, bias=False)

        self.start_index_attention_linear_layer = torch.nn.Linear(in_features=self.embedding.embedding_dim * 2,
                                                                  out_features=self.embedding.embedding_dim * 2,
                                                                  bias=False)

        self.end_index_attention_linear_layer = torch.nn.Linear(in_features=self.embedding.embedding_dim * 2,
                                                                out_features=self.embedding.embedding_dim * 2,
                                                                bias=False)

    def forward(self, question_batch_index_tensor: torch.Tensor, passage_batch_index_tensor: torch.Tensor,
                iteration_num: int = None):
        question_batch = self.embedding(question_batch_index_tensor)
        passage_batch = self.embedding(passage_batch_index_tensor)

        passage_outputs = self.passage_lstm(passage_batch, self._init_lstm_hidden_and_cell_state(
            passage_batch.size()[0], self.embedding.embedding_dim))[0].contiguous()

        # index 1, 0 = h[n] since the lstm output is all_outputs, (h_n, c_n)
        # this, unfortunately is shaped like (num_layers * num_directions, batch, hidden_size)
        # The unfortunate part is that they could've simply concatenated num_directions in the last dimension like
        # they do for all_outputs and that in spite of setting batch_first to be True, we get batch as the 2nd dimension
        question_last_hidden_states: torch.Tensor = \
            self.question_embedding_lstm(question_batch, self._init_lstm_hidden_and_cell_state(
                question_batch.size()[0], self.embedding.embedding_dim))[1][0]

        # let's match the last dimension with passage outputs
        # q > transpose > reshape > expand > contiguous
        question_last_hidden_states = question_last_hidden_states.transpose(dim0=0, dim1=1)
        question_last_hidden_states = question_last_hidden_states.reshape((
            question_last_hidden_states.shape[0], int(question_last_hidden_states.shape[1] / 2),
            question_last_hidden_states.shape[2] * 2))  # .expand(-1, passage_outputs.shape[1], -1).contiguous()

        # (if/when expand doesn't work) repeat the concatenated hidden states along dim 1 to allow using the bilinear attention layer
        # question_last_hidden_states = question_last_hidden_states.repeat(1, passage_outputs.shape[1], 1)

        start_index_attention_scores: torch.Tensor = torch.matmul(passage_outputs, torch.transpose(
            self.start_index_attention_linear_layer(question_last_hidden_states), dim0=1, dim1=2))

        end_index_attention_scores: torch.Tensor = torch.matmul(passage_outputs, torch.transpose(
            self.end_index_attention_linear_layer(question_last_hidden_states), dim0=1, dim1=2))

        assert start_index_attention_scores.shape == torch.Size((passage_outputs.shape[0], passage_outputs.shape[1], 1))
        assert end_index_attention_scores.shape == torch.Size((passage_outputs.shape[0], passage_outputs.shape[1], 1))

        return start_index_attention_scores, end_index_attention_scores
